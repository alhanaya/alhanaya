{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alhanaya/alhanaya/blob/main/Arabic_Text_Classification_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bcf3246",
      "metadata": {
        "id": "9bcf3246"
      },
      "source": [
        "\n",
        "# ğŸ‡¸ğŸ‡¦ Arabic Text Classification (Colab-Ready)\n",
        "**Goal:** Train a small multilingual model to classify Arabic text into 3 sentiments (negative=0, neutral=1, positive=2).  \n",
        "Works outâ€‘ofâ€‘theâ€‘box on **Google Colab** (CPU/GPU).\n",
        "\n",
        "**What you'll get:**\n",
        "- Data setup (either Google Drive or inline sample)\n",
        "- Training with ğŸ¤— Transformers + Datasets\n",
        "- Evaluation (accuracy/F1)\n",
        "- Inference cell to test your own sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9a382a41",
      "metadata": {
        "id": "9a382a41"
      },
      "outputs": [],
      "source": [
        "\n",
        "# âœ… If you're on Google Colab, you can set the runtime to GPU for faster training:\n",
        "# Runtime > Change runtime type > Hardware accelerator: GPU\n",
        "\n",
        "!pip -q install \"transformers>=4.44.0\" \"datasets>=2.20.0\" \"accelerate>=0.30.0\" \"evaluate>=0.4.2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, datasets, accelerate, evaluate\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjklXTuiA9xL",
        "outputId": "57bab9c3-3648-4247-ee6c-e0d47a6eedb9"
      },
      "id": "VjklXTuiA9xL",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers: 4.55.2\n",
            "datasets: 4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f66c8071",
      "metadata": {
        "id": "f66c8071"
      },
      "source": [
        "\n",
        "## Option A) Use your data from Google Drive (JSONL)\n",
        "Your files should be in your Drive, e.g. `/content/drive/MyDrive/ai-train/data/train.jsonl` and `dev.jsonl`, each line like:\n",
        "```json\n",
        "{\"text\":\"Ø§Ù„Ø®Ø¯Ù…Ø© Ø³ÙŠØ¦Ø©\",\"label\":0}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4d37e45f",
      "metadata": {
        "id": "4d37e45f"
      },
      "outputs": [],
      "source": [
        "\n",
        "USE_DRIVE = False  # â¬…ï¸ set to True if your data is on Google Drive\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    TRAIN_PATH = \"/content/drive/MyDrive/ai-train/data/train.jsonl\"\n",
        "    DEV_PATH   = \"/content/drive/MyDrive/ai-train/data/dev.jsonl\"\n",
        "else:\n",
        "    TRAIN_PATH = \"train.jsonl\"\n",
        "    DEV_PATH   = \"dev.jsonl\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "624d6386",
      "metadata": {
        "id": "624d6386"
      },
      "source": [
        "\n",
        "## Option B) Use small inline sample (quick start)\n",
        "Runs even without Drive. You can replace with your own later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "19476725",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19476725",
        "outputId": "9a5d57fc-a9e0-4feb-e4ed-933f9641bae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train file: train.jsonl\n",
            "Dev file  : dev.jsonl\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sample_train = [\n",
        "    {\"text\":\"Ø§Ù„Ø®Ø¯Ù…Ø© Ø³ÙŠØ¦Ø© ÙˆØ§Ù„ØªØ£Ø®ÙŠØ± ØºÙŠØ± Ù…Ù‚Ø¨ÙˆÙ„.\",\"label\":0},\n",
        "    {\"text\":\"Ø§Ù„Ø·Ù„Ø¨ ÙˆØµÙ„ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙˆØ§Ù„Ø¬ÙˆØ¯Ø© Ù…Ù…ØªØ§Ø²Ø©.\",\"label\":2},\n",
        "    {\"text\":\"Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¹Ø§Ø¯ÙŠ ÙˆÙ…Ø§ Ù„Ø§Ø­Ø¸Øª ÙØ±Ù‚ ÙƒØ¨ÙŠØ±.\",\"label\":1},\n",
        "    {\"text\":\"ØªØ¬Ø±Ø¨Ø© Ù…Ø²Ø¹Ø¬Ø© ÙˆØ§Ù„Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙŠ Ù„Ø§ ÙŠØ±Ø¯.\",\"label\":0},\n",
        "    {\"text\":\"Ø§Ù„ØªØºÙ„ÙŠÙ Ø±Ø§Ø¦Ø¹ ÙˆØ§Ù„Ù…Ù†ØªØ¬ Ù…Ø«Ù„ Ø§Ù„ÙˆØµÙ ØªÙ…Ø§Ù…Ù‹Ø§.\",\"label\":2},\n",
        "    {\"text\":\"Ù„Ø§ Ø¬Ø¯ÙŠØ¯ ÙŠØ°ÙƒØ±ØŒ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ù…ØªÙˆØ³Ø·Ø©.\",\"label\":1},\n",
        "    {\"text\":\"Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø®ÙŠØ± Ø³Ø¨Ø¨ Ø£Ø¹Ø·Ø§Ù„ ÙƒØ«ÙŠØ±Ø©.\",\"label\":0},\n",
        "    {\"text\":\"Ø§Ù„Ø³Ø¹Ø± Ù…Ù†Ø§Ø³Ø¨ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.\",\"label\":2},\n",
        "    {\"text\":\"Ø§Ù„Ø®Ø¯Ù…Ø© Ù…Ù‚Ø¨ÙˆÙ„Ø© Ù„ÙƒÙ† ØªØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†.\",\"label\":1},\n",
        "    {\"text\":\"Ø§Ù„ØªØ³Ù„ÙŠÙ… ØªØ£Ø®Ø± ÙƒØ«ÙŠØ±Ø§Ù‹ ÙˆÙ‡Ø°Ø§ Ù…Ø­Ø¨Ø·.\",\"label\":0},\n",
        "    {\"text\":\"Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ù…Ø±ØªØ¨Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù….\",\"label\":2},\n",
        "    {\"text\":\"Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ Ø§Ù„Ø­ÙƒÙ… Ø­Ø§Ù„ÙŠØ§Ù‹ØŒ Ù…Ø§ Ø¬Ø±Ø¨Øª ÙƒÙ„ Ø´ÙŠØ¡.\",\"label\":1}\n",
        "]\n",
        "\n",
        "sample_dev = [\n",
        "    {\"text\":\"Ø¬ÙˆØ¯Ø© Ø±Ø¯ÙŠØ¦Ø© ÙˆØ®Ø¯Ù…Ø© Ø¨Ø·ÙŠØ¦Ø©.\",\"label\":0},\n",
        "    {\"text\":\"Ø§Ù„Ø£Ù…ÙˆØ± Ø·Ø¨ÙŠØ¹ÙŠØ© Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†.\",\"label\":1},\n",
        "    {\"text\":\"Ù†ØªÙŠØ¬Ø© Ù…Ù…ØªØ§Ø²Ø© ÙˆÙØ±ÙŠÙ‚ Ù…Ù…ÙŠØ².\",\"label\":2},\n",
        "    {\"text\":\"ÙˆØ§Ø¬Ù‡Øª Ù…Ø´Ø§ÙƒÙ„ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø¯ÙŠØ«.\",\"label\":0},\n",
        "    {\"text\":\"Ø§Ù„Ø®Ø¯Ù…Ø© Ù„Ø§ Ø¨Ø£Ø³ Ø¨Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù….\",\"label\":1},\n",
        "    {\"text\":\"Ø£Ù†ØµØ­ Ø¨Ù‡ Ø¨Ø´Ø¯Ø©!\",\"label\":2}\n",
        "]\n",
        "\n",
        "# Write files only if not using Drive\n",
        "import json, os\n",
        "if not os.path.exists(TRAIN_PATH):\n",
        "    with open(TRAIN_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for row in sample_train:\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "if not os.path.exists(DEV_PATH):\n",
        "    with open(DEV_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for row in sample_dev:\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Train file:\", TRAIN_PATH)\n",
        "print(\"Dev file  :\", DEV_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6de42eb",
      "metadata": {
        "id": "e6de42eb"
      },
      "source": [
        "\n",
        "## Load dataset\n",
        "We use ğŸ¤— Datasets with JSON Lines format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f39aabb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f39aabb0",
        "outputId": "0616d2fd-85bd-4e97-d4c8-22b5546a6b6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 12\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 6\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "data_files = {\"train\": TRAIN_PATH, \"validation\": DEV_PATH}\n",
        "ds = load_dataset(\"json\", data_files=data_files)\n",
        "ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1060830",
      "metadata": {
        "id": "c1060830"
      },
      "source": [
        "\n",
        "## Tokenization & Model\n",
        "We'll use a lightweight multilingual model: `distilbert-base-multilingual-cased` (good enough for demo).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "    f.write('{\"text\": \"Ø§Ù„Ø®Ø¯Ù…Ø© Ù…Ù…ØªØ§Ø²Ø© ÙˆØ±Ø®ÙŠØµØ©\", \"label\": 2}\\n')\n"
      ],
      "metadata": {
        "id": "WogxwpkTWt4J"
      },
      "id": "WogxwpkTWt4J",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8c313eab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c313eab",
        "outputId": "9d0c8232-946d-44dd-e615-0e9de81c8fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-multilingual-cased\"\n",
        "tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok_fn(batch):\n",
        "    return tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "ds_tok = ds.map(tok_fn, batched=True, remove_columns=[c for c in ds[\"train\"].column_names if c not in [\"text\",\"label\"]])\n",
        "\n",
        "# Ensure 'labels' column exists\n",
        "for split in ds_tok.keys():\n",
        "    if \"label\" in ds_tok[split].column_names and \"labels\" not in ds_tok[split].column_names:\n",
        "        ds_tok[split] = ds_tok[split].rename_column(\"label\", \"labels\")\n",
        "\n",
        "num_labels = 3  # 0=negative, 1=neutral, 2=positive\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86d15771",
      "metadata": {
        "id": "86d15771"
      },
      "source": [
        "\n",
        "## Train\n",
        "One short epoch for demo. Increase epochs and data for better results.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "1IBry2IQbcwB"
      },
      "id": "1IBry2IQbcwB",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "52b547b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "52b547b8",
        "outputId": "f358fb31-0718-4b1e-a2e5-781fc4a6739f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-2090568312.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.0984792709350586,\n",
              " 'eval_accuracy': 0.5,\n",
              " 'eval_f1_macro': 0.43333333333333335,\n",
              " 'eval_runtime': 1.2377,\n",
              " 'eval_samples_per_second': 4.848,\n",
              " 'eval_steps_per_second': 0.808,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"   # ØªØ¹Ø·ÙŠÙ„ wandb\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
        "    }\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"runs/mini\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=20,\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_tok[\"train\"],\n",
        "    eval_dataset=ds_tok[\"validation\"],\n",
        "    tokenizer=tok,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcc54842",
      "metadata": {
        "id": "bcc54842"
      },
      "source": [
        "\n",
        "## Inference\n",
        "Try your own sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0fcd94ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fcd94ff",
        "outputId": "c7e65062-8b88-4ba2-b3b0-4e2574e26e38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'Ø­ÙŠØ§Ø¯ÙŠ']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "id2label = {0:\"Ø³Ù„Ø¨ÙŠ\", 1:\"Ø­ÙŠØ§Ø¯ÙŠ\", 2:\"Ø¥ÙŠØ¬Ø§Ø¨ÙŠ\"}\n",
        "\n",
        "def predict(texts):\n",
        "    enc = tok(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        out = model(**enc)\n",
        "        preds = out.logits.argmax(dim=-1).cpu().tolist()\n",
        "    return [id2label[p] for p in preds]\n",
        "\n",
        "examples = [\n",
        "    \"Ø§Ù„Ø®Ø¯Ù…Ø© Ø³ÙŠØ¦Ø© Ø¬Ø¯Ù‹Ø§ ÙˆØ§Ù„ØªØ£Ø®ÙŠØ± Ù…Ø²Ø¹Ø¬.\",\n",
        "    \"Ø§Ù„ÙˆØ¶Ø¹ Ø¹Ø§Ø¯ÙŠ ÙˆÙ…Ø§ Ø¹Ù†Ø¯ÙŠ ØªØ¹Ù„ÙŠÙ‚.\",\n",
        "    \"Ù…Ù…ØªØ§Ø²! ØªØ¬Ø±Ø¨Ø© Ø±Ø§Ø¦Ø¹Ø© ÙˆØ£Ù†ØµØ­ Ø¨Ù‡Ø§.\"\n",
        "]\n",
        "\n",
        "predict(examples)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}